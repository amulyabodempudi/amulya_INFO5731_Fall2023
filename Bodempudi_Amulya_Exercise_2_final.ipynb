{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amulyabodempudi/amulya_INFO5731_Fall2023/blob/main/Bodempudi_Amulya_Exercise_2_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW5_oFVd9-pY"
      },
      "source": [
        "## The second In-class-exercise (09/13/2023, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kindly use the provided .ipynb document to write your code or respond to the questions. Avoid generating a new file.\n",
        "Execute all the cells before your final submission."
      ],
      "metadata": {
        "id": "mAzh1U0sE5I5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This in-class exercise is due tomorrow September 14, 2023 at 11:59 PM. No late submissions will be considered."
      ],
      "metadata": {
        "id": "PpgvZQdRE-HV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QBZI-je9-pZ"
      },
      "source": [
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWoKpYQT9-pa"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LmNR3kw9-pa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "c9e3656e-d28c-4aa0-c4ab-d4a5221fcd68"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nPlease write you answer here:\\nQuestion:\\nExploring the Impact of Virtual Reality (VR) Therapy on Anxiety and Stress Reduction in Healthcare Settings\\n\\nData Collection:\\nTo answer these research questions, the following types of data need to be collected:\\nPatient Demographics: Collect data on patients' age, gender, medical history, and previous experiences with VR if any.\\nAnxiety Assessment: Use standardized anxiety assessment tools such as the State-Trait Anxiety Inventory (STAI) or the Hospital Anxiety and Depression Scale (HADS) to measure anxiety levels before and after VR therapy sessions.\\nVR Therapy Sessions: Record data on the type of VR scenarios used, session duration, and any adverse reactions or discomfort reported by patients during the sessions.\\nMedical Procedure Data: For patients undergoing medical procedures, collect data on the type of procedure, its duration, and any complications or difficulties experienced by healthcare providers during the procedure.\\nFollow-up Surveys: Administer follow-up surveys to patients after a designated period to assess the long-term impact of VR therapy on anxiety and stress levels.\\n\\nSample Size:\\nThe required sample size will depend on the statistical power desired and the effect size expected. To ensure meaningful results, aim for a sample size of at least 100 patients. A larger sample size may be necessary for subgroup analysis or to detect smaller effect sizes.\\n\\nData Collection Steps:\\n\\nEthical Approval: Obtain ethical approval from relevant institutional review boards or ethics committees.\\nRecruitment: Identify eligible patients in healthcare settings, such as hospitals or clinics, and obtain informed consent for participation in the study.\\nBaseline Data Collection: Collect baseline data on patient demographics and anxiety levels using standardized assessment tools.\\nVR Therapy Sessions: Administer VR therapy sessions according to a standardized protocol. Record session details and monitor patient reactions.\\nPost-Session Data: Collect post-session anxiety data immediately after the VR therapy sessions.\\nMedical Procedure Data: For patients undergoing medical procedures, collect data on the procedure and any relevant complications.\\nFollow-up Surveys: Administer follow-up surveys at predetermined intervals to assess the long-term impact of VR therapy.\\nData Storage: Ensure secure storage of collected data, adhering to data protection regulations.\\nData Analysis: Analyze the data using appropriate statistical methods, such as t-tests, ANOVA, regression analysis, or machine learning techniques, depending on the research questions and data types.\\nReporting: Summarize the findings in a research paper, including statistical results, conclusions, and recommendations.\\n\\nCollecting and analyzing this data will provide valuable insights into the effectiveness of VR therapy for anxiety and stress reduction in healthcare settings, potentially leading to innovative approaches for improving patient well-being.\\n\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "Question:\n",
        "Exploring the Impact of Virtual Reality (VR) Therapy on Anxiety and Stress Reduction in Healthcare Settings\n",
        "\n",
        "Data Collection:\n",
        "To answer these research questions, the following types of data need to be collected:\n",
        "Patient Demographics: Collect data on patients' age, gender, medical history, and previous experiences with VR if any.\n",
        "Anxiety Assessment: Use standardized anxiety assessment tools such as the State-Trait Anxiety Inventory (STAI) or the Hospital Anxiety and Depression Scale (HADS) to measure anxiety levels before and after VR therapy sessions.\n",
        "VR Therapy Sessions: Record data on the type of VR scenarios used, session duration, and any adverse reactions or discomfort reported by patients during the sessions.\n",
        "Medical Procedure Data: For patients undergoing medical procedures, collect data on the type of procedure, its duration, and any complications or difficulties experienced by healthcare providers during the procedure.\n",
        "Follow-up Surveys: Administer follow-up surveys to patients after a designated period to assess the long-term impact of VR therapy on anxiety and stress levels.\n",
        "\n",
        "Sample Size:\n",
        "The required sample size will depend on the statistical power desired and the effect size expected. To ensure meaningful results, aim for a sample size of at least 100 patients. A larger sample size may be necessary for subgroup analysis or to detect smaller effect sizes.\n",
        "\n",
        "Data Collection Steps:\n",
        "\n",
        "Ethical Approval: Obtain ethical approval from relevant institutional review boards or ethics committees.\n",
        "Recruitment: Identify eligible patients in healthcare settings, such as hospitals or clinics, and obtain informed consent for participation in the study.\n",
        "Baseline Data Collection: Collect baseline data on patient demographics and anxiety levels using standardized assessment tools.\n",
        "VR Therapy Sessions: Administer VR therapy sessions according to a standardized protocol. Record session details and monitor patient reactions.\n",
        "Post-Session Data: Collect post-session anxiety data immediately after the VR therapy sessions.\n",
        "Medical Procedure Data: For patients undergoing medical procedures, collect data on the procedure and any relevant complications.\n",
        "Follow-up Surveys: Administer follow-up surveys at predetermined intervals to assess the long-term impact of VR therapy.\n",
        "Data Storage: Ensure secure storage of collected data, adhering to data protection regulations.\n",
        "Data Analysis: Analyze the data using appropriate statistical methods, such as t-tests, ANOVA, regression analysis, or machine learning techniques, depending on the research questions and data types.\n",
        "Reporting: Summarize the findings in a research paper, including statistical results, conclusions, and recommendations.\n",
        "\n",
        "Collecting and analyzing this data will provide valuable insights into the effectiveness of VR therapy for anxiety and stress reduction in healthcare settings, potentially leading to innovative approaches for improving patient well-being.\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlxTLRNm9-pa"
      },
      "source": [
        "Question 2 (10 points): Write python code to collect 1000 data samples you discussed above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpWOgjHi9-pa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd7f2dcb-455d-4941-b651-513eda5345c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Patient_ID  Age  Gender Medical_History VR_Session_Type  Session_Duration  \\\n",
            "0            1   44   Other    Hypertension     Distraction                32   \n",
            "1            2   28  Female          Asthma      Relaxation                10   \n",
            "2            3   68  Female        Diabetes     Distraction                28   \n",
            "3            4   45    Male            None      Relaxation                57   \n",
            "4            5   24   Other          Asthma     Distraction                40   \n",
            "..         ...  ...     ...             ...             ...               ...   \n",
            "95          96   30   Other        Diabetes      Relaxation                50   \n",
            "96          97   73    Male        Diabetes     Distraction                22   \n",
            "97          98   80   Other           Other      Relaxation                44   \n",
            "98          99   59  Female          Asthma      Relaxation                20   \n",
            "99         100   73  Female          Asthma      Relaxation                38   \n",
            "\n",
            "    Pre_Session_Anxiety  Post_Session_Anxiety Procedure_Type  \\\n",
            "0                     2                     8        Surgery   \n",
            "1                     8                     2           None   \n",
            "2                     1                     3           None   \n",
            "3                     8                    10           None   \n",
            "4                    10                     8           None   \n",
            "..                  ...                   ...            ...   \n",
            "95                    4                     6           None   \n",
            "96                    1                     1           None   \n",
            "97                    6                     3        Surgery   \n",
            "98                    9                     9          X-ray   \n",
            "99                    6                     5          X-ray   \n",
            "\n",
            "    Procedure_Duration Complications  \n",
            "0                 24.0          None  \n",
            "1                  NaN          None  \n",
            "2                  NaN          None  \n",
            "3                  NaN          None  \n",
            "4                  NaN          None  \n",
            "..                 ...           ...  \n",
            "95                 NaN          None  \n",
            "96                 NaN          None  \n",
            "97                71.0         Minor  \n",
            "98               124.0         Major  \n",
            "99               180.0          None  \n",
            "\n",
            "[100 rows x 11 columns]\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "# Create an empty list to store data for each patient\n",
        "data_list = []\n",
        "\n",
        "# Simulate data collection for 100 patients\n",
        "for patient_id in range(1, 1001):\n",
        "    # Simulate patient demographics\n",
        "    age = random.randint(18, 80)\n",
        "    gender = random.choice(['Male', 'Female', 'Other'])\n",
        "    medical_history = random.choice(['None', 'Hypertension', 'Diabetes', 'Asthma', 'Other'])\n",
        "\n",
        "    # Simulate VR session data\n",
        "    vr_session_type = random.choice(['Relaxation', 'Distraction', 'Therapeutic'])\n",
        "    session_duration = random.randint(10, 60)  # Minutes\n",
        "    pre_session_anxiety = random.randint(0, 10)\n",
        "    post_session_anxiety = random.randint(0, 10)\n",
        "\n",
        "    # Simulate medical procedure data for a subset of patients\n",
        "    if random.random() < 0.2:\n",
        "        procedure_type = random.choice(['Surgery', 'MRI', 'X-ray', 'Blood Test'])\n",
        "        procedure_duration = random.randint(20, 180)  # Minutes\n",
        "        complications = random.choice(['None', 'Minor', 'Major'])\n",
        "\n",
        "    else:\n",
        "        procedure_type = procedure_duration = complications = None\n",
        "\n",
        "    # Append data for the current patient as a dictionary to the list\n",
        "    data_list.append({\n",
        "        'Patient_ID': patient_id,\n",
        "        'Age': age,\n",
        "        'Gender': gender,\n",
        "        'Medical_History': medical_history,\n",
        "        'VR_Session_Type': vr_session_type,\n",
        "        'Session_Duration': session_duration,\n",
        "        'Pre_Session_Anxiety': pre_session_anxiety,\n",
        "        'Post_Session_Anxiety': post_session_anxiety,\n",
        "        'Procedure_Type': procedure_type,\n",
        "        'Procedure_Duration': procedure_duration,\n",
        "        'Complications': complications\n",
        "    })\n",
        "\n",
        "# Create a DataFrame by concatenating the list of patient data\n",
        "data = pd.DataFrame(data_list)\n",
        "\n",
        "# Display the first few rows of the collected data\n",
        "print(data.head(100))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for 2nd question when i tried to scrape from the flipkart website\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Define the URL to scrape\n",
        "url = \"https://www.flipkart.com/audio-video/headphones/pr?sid=0pm%2Cfcn&otracker=categorytree&p%5B%5D=facets.connectivity%255B%255D%3DBluetooth&fm=neo%2Fmerchandising&iid=M_b16cde5a-deeb-4540-8bdb-d5cd627727af_1_372UD5BXDFYS_MC.R08R6GB1Q1BI&otracker=hp_rich_navigation_2_1.navigationCard.RICH_NAVIGATION_Electronics~Audio~Bluetooth%2BHeadphones_R08R6GB1Q1BI&otracker1=hp_rich_navigation_PINNED_neo%2Fmerchandising_NA_NAV_EXPANDABLE_navigationCard_cc_2_L2_view-all&cid=R08R6GB1Q1BI\"\n",
        "\n",
        "# Send an HTTP GET request to the URL\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful (status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Parse the HTML content of the page using BeautifulSoup\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    # Find and extract the product details\n",
        "    product_details = []\n",
        "    products = soup.find_all(\"div\", class_=\"_1AtVbE\")\n",
        "    for product in products:\n",
        "        title_elem = product.find(\"a\", class_=\"_4HTuuX\")\n",
        "        price_elem = product.find(\"div\", class_=\"_8VNy32\")\n",
        "        rating_elem = product.find(\"div\", class_=\"gUuXy- _2D5lwg\")\n",
        "\n",
        "        if title_elem and price_elem and rating_elem:\n",
        "            title = title_elem.get(\"title\")\n",
        "            price = price_elem.text\n",
        "            rating = rating_elem.text\n",
        "            product_details.append({\"Title\": title, \"Price\": price, \"Rating\": rating})\n",
        "\n",
        "    # Create a DataFrame to store the collected data\n",
        "    data = pd.DataFrame(product_details)\n",
        "\n",
        "    # Display the first few rows of the collected data\n",
        "    print(data.head())\n",
        "\n",
        "else:\n",
        "    print(f\"Failed to retrieve data. Status code: {response.status_code}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_x2iI8DNatf",
        "outputId": "0e3d0669-ed43-4c4e-c582-bc1c1a6df808"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px6wgvog9-pa"
      },
      "source": [
        "Question 3 (10 points): Write python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"information retrieval\". The articles should be published in the last 10 years (2013-2023).\n",
        "\n",
        "The following information of the article needs to be collected:\n",
        "\n",
        "(1) Title\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5rjlclf9-pb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e866b566-cfa0-4689-ed17-7835a56238a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected 10 articles\n",
            "Collected 20 articles\n",
            "Article 1:\n",
            "Title: Information retrieval as statistical translation\n",
            "Year: 2017 - dl.acm.org\n",
            "Authors: A Berger, J Lafferty - ACM SIGIR Forum\n",
            "Abstract: … There is a large literature on probabilistic approaches to information retrieval, and we will \n",
            "not attempt to survey it here. Instead, we focus on the language modeling approach introduced …\n",
            "==================================================\n",
            "Article 2:\n",
            "Title: [BOOK][B] Information retrieval: Implementing and evaluating search engines\n",
            "Year: GV Cormack - 2016 - books.google.com\n",
            "Authors: S Buttcher, CLA Clarke\n",
            "Abstract: … Information retrieval forms the foundation for modern search engines. In this textbook we \n",
            "provide an introduction to information retrieval targeted at graduate students and working …\n",
            "==================================================\n",
            "Article 3:\n",
            "Title: A language modeling approach to information retrieval\n",
            "Year: 2017 - dl.acm.org\n",
            "Authors: JM Ponte, WB Croft - ACM SIGIR Forum\n",
            "Abstract: … models, we have developed an approach to retrieval based on probabilistic language … in \n",
            "information retrieval in two senses. The first sense denotes an abstraction of the retrieval task …\n",
            "==================================================\n",
            "Article 4:\n",
            "Title: A study of smoothing methods for language models applied to ad hoc information retrieval\n",
            "Year: 2017 - dl.acm.org\n",
            "Authors: C Zhai, J Lafferty - ACM SIGIR Forum\n",
            "Abstract: … to information retrieval are attractive and promising because they connect the problem of \n",
            "retrieval … smoothing and its influence on retrieval performance. We examine the sensitivity of …\n",
            "==================================================\n",
            "Article 5:\n",
            "Title: A latent semantic model with convolutional-pooling structure for information retrieval\n",
            "Year: 2014 - dl.acm.org\n",
            "Authors: Y Shen, X He, J Gao, L Deng, G Mesnil - … on conference on information …\n",
            "Abstract: … lexical matching for Web document retrieval. This is partially … have also been proposed for \n",
            "information retrieval (IR) [16][32]… as weakly-supervised information in training the model. In …\n",
            "==================================================\n",
            "Article 6:\n",
            "Title: Integrating and evaluating neural word embeddings in information retrieval\n",
            "Year: 2015 - dl.acm.org\n",
            "Authors: G Zuccon, B Koopman, P Bruza… - Proceedings of the 20th …\n",
            "Abstract: … in information retrieval. Specifically, we first show how word embeddings can be incorporated \n",
            "into a retrieval … building word embeddings have on retrieval effectiveness. The empirical …\n",
            "==================================================\n",
            "Article 7:\n",
            "Title: Deep sentence embedding using long short-term memory networks: Analysis and application to information retrieval\n",
            "Year: 2016 - ieeexplore.ieee.org\n",
            "Authors: H Palangi, L Deng, Y Shen, J Gao, X He… - … on Audio, Speech …\n",
            "Abstract: … Besides, we also include in Table IV two well-known information retrieval (IR) models, \n",
            "BM25 and PLSA, for the sake of benchmarking. The BM25 model uses the bag-of-words …\n",
            "==================================================\n",
            "Article 8:\n",
            "Title: [PDF][PDF] Leveraging linguistic structure for open domain information extraction\n",
            "Year: 2015 - aclanthology.org\n",
            "Authors: G Angeli, MJJ Premkumar… - Proceedings of the 53rd …\n",
            "Abstract: Relation triples produced by open domain information extraction (open IE) systems are \n",
            "useful for question answering, inference, and other IE tasks. Traditionally these are extracted …\n",
            "==================================================\n",
            "Article 9:\n",
            "Title: [BOOK][B] Text information retrieval systems\n",
            "Year: DH Kraft… - 2017 - repo.iainbatusangkar.ac.id\n",
            "Authors: BR Boyce, BR Boyce, CT Meadow, DH Kraft\n",
            "Abstract: \"Information retrieval is a communication process that links an information user or seeker to … \n",
            "and retrieval. This book's purpose is to teach people who will be searching or designing text …\n",
            "==================================================\n",
            "Article 10:\n",
            "Title: Neural approaches to conversational AI\n",
            "Year: 2018 - dl.acm.org\n",
            "Authors: J Gao, M Galley, L Li - … research & development in information retrieval\n",
            "Abstract: This tutorial surveys neural approaches to conversational AI that were developed in the last \n",
            "few years. We group conversational systems into three categories: (1) question answering …\n",
            "==================================================\n",
            "Total 20 articles collected.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "# Set the base URL and search query\n",
        "\n",
        "base_url = \"https://scholar.google.com/scholar\"\n",
        "\n",
        "query = \"information retrieval\"\n",
        "\n",
        "params = {\n",
        "\n",
        "    \"q\": query,\n",
        "\n",
        "    \"as_vis\": 0,  # Disable \"Search within citing articles\"\n",
        "\n",
        "    \"as_sdt\": 0,  # Disable \"Since Year\"\n",
        "\n",
        "    \"as_ylo\": 2013,  # Start year\n",
        "\n",
        "    \"as_yhi\": 2023,  # End year\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "# Function to extract article information\n",
        "\n",
        "def extract_article_info(article):\n",
        "\n",
        "    title = article.select_one(\"h3\").text\n",
        "\n",
        "    authors_and_year = article.select(\".gs_a\")[0].text.strip()\n",
        "\n",
        "\n",
        "\n",
        "    authors_and_year_parts = authors_and_year.rsplit(',', 1)\n",
        "\n",
        "\n",
        "\n",
        "    authors = authors_and_year_parts[0].strip()\n",
        "\n",
        "\n",
        "\n",
        "    if len(authors_and_year_parts) > 1:\n",
        "\n",
        "        year = authors_and_year_parts[1].strip()\n",
        "\n",
        "    else:\n",
        "\n",
        "        year = \"N/A\"\n",
        "    abstract = article.select_one(\".gs_rs\").text.strip()\n",
        "    return {\n",
        "\n",
        "        \"Title\": title,\n",
        "\n",
        "        \"Year\": year,\n",
        "\n",
        "        \"Authors\": authors,\n",
        "\n",
        "        \"Abstract\": abstract,\n",
        "\n",
        "    }\n",
        "# Scrape the search results\n",
        "\n",
        "articles_list = []\n",
        "# Initial delay\n",
        "\n",
        "time.sleep(10)  # Increase the initial delay to 10 seconds\n",
        "for start in range(0, 20, 10):  # Collect only 2 pages of articles (10 per page)\n",
        "\n",
        "    params[\"start\"] = start\n",
        "\n",
        "    response = requests.get(base_url, params=params)\n",
        "    if response.status_code == 429:\n",
        "\n",
        "        print(f\"Rate limited. Waiting for 60 seconds...\")\n",
        "\n",
        "        time.sleep(60)  # Wait for 60 seconds if rate-limited\n",
        "\n",
        "        continue  # Retry the same request\n",
        "    if response.status_code != 200:\n",
        "\n",
        "        print(f\"Error accessing Google Scholar - Status Code: {response.status_code}\")\n",
        "\n",
        "        break\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    articles = soup.select(\".gs_ri\")\n",
        "\n",
        "    for article in articles:\n",
        "\n",
        "        article_info = extract_article_info(article)\n",
        "\n",
        "        articles_list.append(article_info)\n",
        "\n",
        "    # Display progress\n",
        "\n",
        "    print(f\"Collected {len(articles_list)} articles\")\n",
        "    # Add a delay to avoid rate limiting\n",
        "\n",
        "    time.sleep(10)  # Increase the delay to 10 seconds\n",
        "# Display the first few articles\n",
        "\n",
        "for i, article_info in enumerate(articles_list[:10], start=1):\n",
        "\n",
        "    print(f\"Article {i}:\")\n",
        "\n",
        "    print(f\"Title: {article_info['Title']}\")\n",
        "\n",
        "    print(f\"Year: {article_info['Year']}\")\n",
        "\n",
        "    print(f\"Authors: {article_info['Authors']}\")\n",
        "\n",
        "    print(f\"Abstract: {article_info['Abstract']}\")\n",
        "\n",
        "    print(\"=\" * 50)\n",
        "# Display total number of articles collected\n",
        "\n",
        "print(f\"Total {len(articles_list)} articles collected.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "_*Do either of the question-4 tasks given below."
      ],
      "metadata": {
        "id": "yCQpbJnwTxAB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT3CNj_V9-pb"
      },
      "source": [
        "Question 4 (10 points): Write python code to collect 1000 posts from Twitter, or Facebook, or Instagram. You can either use hashtags, keywords, user_name, user_id, or other information to collect the data.\n",
        "\n",
        "The following information needs to be collected:\n",
        "\n",
        "(1) User_name\n",
        "\n",
        "(2) Posted time\n",
        "\n",
        "(3) Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FymVNKVi9-pb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "360fd64f-8ba9-4f50-ff97-a87a0569fc12"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "Forbidden",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mForbidden\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-cff25d7de10c>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_tweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeyword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"extended\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_tweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0muser_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscreen_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mposted_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreated_at\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0;31m# Reached end of current page, get the next page...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRawParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             model = ModelParser().parse(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tweepy/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpagination_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tweepy/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'payload_list'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpayload_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'payload_type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpayload_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpayload_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpayload_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpayload_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpayload_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tweepy/api.py\u001b[0m in \u001b[0;36msearch_tweets\u001b[0;34m(self, q, **kwargs)\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0m_Twitter\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mdocumentation\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mstandard\u001b[0m \u001b[0msearch\u001b[0m \u001b[0mAPI\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mdeveloper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtwitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcom\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0men\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtwitter\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0moverview\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \"\"\"\n\u001b[0;32m-> 1309\u001b[0;31m         return self.request(\n\u001b[0m\u001b[1;32m   1310\u001b[0m             'GET', 'search/tweets', endpoint_parameters=(\n\u001b[1;32m   1311\u001b[0m                 \u001b[0;34m'q'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'geocode'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lang'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'locale'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'result_type'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'count'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tweepy/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, endpoint, endpoint_parameters, params, headers, json_payload, parser, payload_list, payload_type, post_data, files, require_auth, return_cursors, upload_api, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mUnauthorized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m403\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mForbidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m404\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mNotFound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mForbidden\u001b[0m: 403 Forbidden\n453 - You currently have access to a subset of Twitter API v2 endpoints and limited v1.1 endpoints (e.g. media post, oauth) only. If you need access to this endpoint, you may need a different access level. You can learn more here: https://developer.twitter.com/en/portal/product"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "import tweepy\n",
        "\n",
        "# Set your Twitter API credentials\n",
        "consumer_key = \"ziUfa9kVnR26WXglyvjimtgri\"\n",
        "consumer_secret = \"xID6jqGP3QrLsyT7SAApgZznsluhSyOGkDLsREOdvhld29upps\"\n",
        "access_token = \"1621735299960250368-TRTprmTCIeE5yOWDktTnc0LSH0R3RT\"\n",
        "access_token_secret = \"VTu35BPYTsWCt6RO4QjW2v8izB8BaOLOFM7yW1Wu6a0up\"\n",
        "\n",
        "# Authenticate with Twitter API\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "\n",
        "# Create an API object\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "# Define the keyword you want to search for\n",
        "keyword = \"AP ELECTION\"\n",
        "max_tweets = 1000\n",
        "\n",
        "# Collect tweets\n",
        "tweets = []\n",
        "\n",
        "for tweet in tweepy.Cursor(api.search_tweets, q=keyword, lang=\"en\", tweet_mode=\"extended\").items(max_tweets):\n",
        "    user_name = tweet.user.screen_name\n",
        "    posted_time = tweet.created_at\n",
        "    text = tweet.full_text\n",
        "\n",
        "    tweets.append({\n",
        "        \"User_name\": user_name,\n",
        "        \"Posted_time\": posted_time,\n",
        "        \"Text\": text\n",
        "    })\n",
        "\n",
        "# Print the first 5 collected tweets as an example\n",
        "for i, tweet in enumerate(tweets[:5], start=1):\n",
        "    print(f\"Tweet {i}:\")\n",
        "    print(f\"User_name: {tweet['User_name']}\")\n",
        "    print(f\"Posted_time: {tweet['Posted_time']}\")\n",
        "    print(f\"Text: {tweet['Text']}\")\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4 (10 points):\n",
        "\n",
        "In this task, you are required to identify and utilize online tools for web scraping data from websites without the need for coding, with a specific focus on Parsehub. The objective is to gather data and save it in formats like CSV, Excel, or any other suitable file format.\n",
        "\n",
        "You have to mention an introduction to the tool which ever you prefer to use, steps to follow for web scrapping and the final output of the data collected.\n",
        "\n",
        "Upload a document (Word or PDF File) in the same repository and you can add the link in the ipynb file."
      ],
      "metadata": {
        "id": "wOeAr9TJTBgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the link to the document here\n",
        "https://github.com/amulyabodempudi/amulya_INFO5731_Fall2023/blob/main/Bodempudi_Amulya_exercise_2.pdf\n"
      ],
      "metadata": {
        "id": "N20TjXLmTG1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7P-xB3TqJy6s"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}