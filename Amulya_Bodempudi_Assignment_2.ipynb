{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amulyabodempudi/amulya_INFO5731_Fall2023/blob/main/Amulya_Bodempudi_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Two**\n",
        "\n",
        "In this assignment, you will try to gather text data from open data source via web scraping or API. After that you need to clean the text data and syntactic analysis of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(40 points). Write a python program to collect text data from **either of the following sources** and save the data into a **csv file**:\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon.\n",
        "\n",
        "(2) Collect the top 10000 User Reviews of a film recently in 2023 or 2022 (you can choose any film) from IMDB.\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from [G2](https://www.g2.com/) or [Capterra](https://www.capterra.com/)\n",
        "\n",
        "(4) Collect the abstracts of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from [Semantic Scholar](https://www.semanticscholar.org).\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the [Densho Digital Repository](https://ddr.densho.org/narrators/).\n",
        "\n",
        "(6) Collect the top 10000 reddits by using a hashtag (you can use any hashtag) from Reddits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PuFPKhC0m1fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd1b4289-72b6-4c34-e099-8800b38e13be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully collected and saved 10000 reviews for movie Johnwick:Chapter 4.\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "# I had choosen 2nd question for collecting 10000 reviews of \"Johnwick\" movie from IMBD\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "def scrape_imdb_reviews(movie_id, num_reviews=10000):\n",
        "    base_url = f'https://www.imdb.com/title/{movie_id}/reviews?ref_=tt_ql_3'\n",
        "\n",
        "    reviews = []\n",
        "    page_number = 1\n",
        "\n",
        "    while len(reviews) < num_reviews:\n",
        "        url = f\"{base_url}&start={page_number}\"\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        review_elements = soup.find_all('div', class_='text show-more__control')\n",
        "\n",
        "        if not review_elements:\n",
        "            break\n",
        "\n",
        "        for review in review_elements:\n",
        "            reviews.append(review.text.strip())\n",
        "\n",
        "        page_number += 1\n",
        "\n",
        "    return reviews[:num_reviews]\n",
        "\n",
        "def save_to_csv(data, filename='imdb_reviews.csv'):\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "        writer = csv.writer(csv_file)\n",
        "        writer.writerow(['Review'])\n",
        "        for review in data:\n",
        "            writer.writerow([review])\n",
        "\n",
        "# Specify the movie ID for \"tt10366206\"\n",
        "movie_id = 'tt10366206'\n",
        "movie_reviews = scrape_imdb_reviews(movie_id, num_reviews=10000)\n",
        "\n",
        "if movie_reviews:\n",
        "    save_to_csv(movie_reviews)\n",
        "    print(f\"Successfully collected and saved {len(movie_reviews)} reviews for movie Johnwick:Chapter 4.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points). Write a python program to **clean the text data** you collected above and save the data in a new column in the csv file. The data cleaning steps include:\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the [stopwords list](https://gist.github.com/sebleier/554280).\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vATjQNTY8buA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "325a057d-f941-4a6d-f1b5-86139cb54759"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "#importing the required libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def clean_text(text):\n",
        "    # (1) Remove noise (special characters and punctuations)\n",
        "    text = ''.join([char for char in text if char.isalnum() or char.isspace()])\n",
        "\n",
        "    # (2) Remove numbers\n",
        "    text = ''.join([char for char in text if not char.isdigit()])\n",
        "\n",
        "    # (3) Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    text = ' '.join([word for word in tokens if word.lower() not in stop_words])\n",
        "\n",
        "    # (4) Lowercase all texts\n",
        "    text = text.lower()\n",
        "\n",
        "    # (5) Stemming\n",
        "    porter_stemmer = PorterStemmer()\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    text = ' '.join([porter_stemmer.stem(word) for word in tokens])\n",
        "\n",
        "    # (6) Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    text = ' '.join([lemmatizer.lemmatize(word) for word in tokens])\n",
        "\n",
        "    return text\n",
        "\n",
        "def scrape_imdb_reviews(movie_id, num_reviews=10000):\n",
        "    base_url = f'https://www.imdb.com/title/{movie_id}/reviews?ref_=tt_ql_3'\n",
        "\n",
        "    reviews = []\n",
        "    page_number = 1\n",
        "\n",
        "    while len(reviews) < num_reviews:\n",
        "        url = f\"{base_url}&start={page_number}\"\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        review_elements = soup.find_all('div', class_='text show-more__control')\n",
        "\n",
        "        if not review_elements:\n",
        "            break\n",
        "\n",
        "        for review in review_elements:\n",
        "            cleaned_review = clean_text(review.text.strip())\n",
        "            reviews.append(cleaned_review)\n",
        "\n",
        "        page_number += 1\n",
        "\n",
        "    return reviews[:num_reviews]\n",
        "\n",
        "def save_to_csv(data, filename='imdb_reviews_cleaned.csv'):\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "        writer = csv.writer(csv_file)\n",
        "        writer.writerow(['Original Review', 'Cleaned Review'])\n",
        "        for original_review, cleaned_review in zip(data, [clean_text(review) for review in data]):\n",
        "            writer.writerow([original_review, cleaned_review])\n",
        "\n",
        "# Specify the IMDb movie ID for \"tt10366206\"\n",
        "movie_id = 'tt10366206'\n",
        "movie_reviews = scrape_imdb_reviews(movie_id, num_reviews=10000)\n",
        "save_to_csv(movie_reviews)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(30 points). Write a python program to conduct **syntax and structure analysis** of the clean text you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) Parts of Speech (POS) Tagging: Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) Named Entity Recognition: Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QQKnPjPDHJHr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6681042-65c2-4aea-b1bc-485d25a4caca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Parts of Speech (POS) Tagging:\n",
            "[('imagin', 'NN'), ('video', 'NN'), ('game', 'NN'), ('shoot', 'NN'), ('bad', 'JJ'), ('guy', 'NN'), ('hardwar', 'NN'), ('old', 'JJ'), ('everyth', 'NN'), ('kind', 'NN'), ('slow', 'JJ'), ('focu', 'NN'), ('oppon', 'NN'), ('set', 'VBN'), ('easi', 'JJ'), ('instal', 'JJ'), ('hack', 'NN'), ('give', 'VBP'), ('invinc', 'NN'), ('autoaim', 'NN'), ('come', 'VBP'), ('slowli', 'NN'), ('shout', 'NN'), ('open', 'JJ'), ('weapon', 'NN'), ('fire', 'NN'), ('three', 'CD'), ('four', 'CD'), ('bullet', 'NN'), ('run', 'VB'), ('shoot', 'JJ'), ('anyth', 'NN'), ('anyway', 'RB'), ('use', 'JJ'), ('sniper', 'JJ'), ('explo', 'NN'), ('trap', 'NN'), ('kind', 'NN'), ('cant', 'JJ'), ('even', 'RB'), ('hit', 'VBP'), ('theyr', 'JJ'), ('next', 'JJ'), ('wield', 'NN'), ('knife', 'NN'), ('tri', 'NN'), ('fight', 'NN'), ('decent', 'NN'), ('manner', 'NN'), ('yet', 'RB'), ('avatar', 'JJ'), ('move', 'NN'), ('like', 'IN'), ('year', 'NN'), ('old', 'JJ'), ('man', 'NN'), ('even', 'RB'), ('autoaim', 'VBZ'), ('still', 'RB'), ('suck', 'VBN'), ('somehow', 'RB'), ('get', 'VB'), ('mani', 'JJ'), ('separ', 'JJ'), ('level', 'NN'), ('noth', 'DT'), ('make', 'VBP'), ('sen', 'NN'), ('get', 'VB'), ('big', 'JJ'), ('cinemat', 'NNS'), ('take', 'VBP'), ('cinemat', 'NN'), ('k', 'NN'), ('whatev', 'VBP'), ('feel', 'NN'), ('like', 'IN'), ('slow', 'JJ'), ('motionthat', 'FW'), ('friend', 'VBP'), ('experi', 'JJ'), ('watch', 'NN'), ('jw', 'NN'), ('style', 'NN'), ('peopl', 'NN'), ('applaud', 'NN'), ('first', 'RB'), ('movi', 'JJ'), ('turn', 'NN'), ('dement', 'JJ'), ('caricatur', 'NN'), ('fight', 'VBD'), ('scene', 'NN'), ('ridicul', 'NN'), ('bad', 'JJ'), ('action', 'NN'), ('charact', 'NN'), ('make', 'VBP'), ('sen', 'JJ'), ('world', 'NN'), ('describ', 'NN'), ('make', 'VBP'), ('sen', 'VBN'), ('even', 'RB'), ('tongueincheek', 'JJ'), ('refer', 'VBP'), ('matrix', 'JJ'), ('dont', 'NN'), ('hit', 'VBD'), ('know', 'JJ'), ('enjoy', 'NN'), ('film', 'NN'), ('make', 'VBP'), ('feel', 'NN'), ('old', 'JJ'), ('film', 'NN'), ('three', 'CD'), ('hour', 'NN'), ('irrelevanceand', 'NN'), ('among', 'IN'), ('peopl', 'JJ'), ('cant', 'JJ'), ('understand', 'NN'), ('rate', 'NN'), ('film', 'NN'), ('anyth', 'IN'), ('bore', 'NN'), ('watch', 'NN'), ('end', 'NN'), ('credit', 'NN'), ('scene', 'NN'), ('destroy', 'VBP'), ('littl', 'NN'), ('joy', 'NN'), ('might', 'MD'), ('kept', 'VB'), ('watch', 'NN'), ('thisbottom', 'NN'), ('line', 'NN'), ('mani', 'FW'), ('talent', 'NN'), ('peopl', 'NN'), ('work', 'NN'), ('film', 'NN'), ('board', 'NN'), ('imagin', 'NN'), ('wor', 'VBP'), ('outcom', 'RB'), ('even', 'RB'), ('funni', 'VBP'), ('sad', 'JJ')]\n",
            "POS Counts: Counter({'NN': 75, 'JJ': 30, 'VBP': 14, 'RB': 10, 'VB': 4, 'IN': 4, 'VBN': 3, 'CD': 3, 'FW': 2, 'VBD': 2, 'VBZ': 1, 'DT': 1, 'NNS': 1, 'MD': 1})\n",
            "Constituency Parsing Tree:\n",
            "(S\n",
            "  imagin/NN\n",
            "  video/NN\n",
            "  game/NN\n",
            "  shoot/NN\n",
            "  bad/JJ\n",
            "  guy/NN\n",
            "  hardwar/NN\n",
            "  old/JJ\n",
            "  everyth/NN\n",
            "  kind/NN\n",
            "  slow/JJ\n",
            "  focu/NN\n",
            "  oppon/NN\n",
            "  set/VBN\n",
            "  easi/JJ\n",
            "  instal/JJ\n",
            "  hack/NN\n",
            "  give/VBP\n",
            "  invinc/NN\n",
            "  autoaim/NN\n",
            "  come/VBP\n",
            "  slowli/NN\n",
            "  shout/NN\n",
            "  open/JJ\n",
            "  weapon/NN\n",
            "  fire/NN\n",
            "  three/CD\n",
            "  four/CD\n",
            "  bullet/NN\n",
            "  run/VB\n",
            "  shoot/JJ\n",
            "  anyth/NN\n",
            "  anyway/RB\n",
            "  use/JJ\n",
            "  sniper/JJ\n",
            "  explo/NN\n",
            "  trap/NN\n",
            "  kind/NN\n",
            "  cant/JJ\n",
            "  even/RB\n",
            "  hit/VBP\n",
            "  theyr/JJ\n",
            "  next/JJ\n",
            "  wield/NN\n",
            "  knife/NN\n",
            "  tri/NN\n",
            "  fight/NN\n",
            "  decent/NN\n",
            "  manner/NN\n",
            "  yet/RB\n",
            "  avatar/JJ\n",
            "  move/NN\n",
            "  like/IN\n",
            "  year/NN\n",
            "  old/JJ\n",
            "  man/NN\n",
            "  even/RB\n",
            "  autoaim/VBZ\n",
            "  still/RB\n",
            "  suck/VBN\n",
            "  somehow/RB\n",
            "  get/VB\n",
            "  mani/JJ\n",
            "  separ/JJ\n",
            "  level/NN\n",
            "  noth/DT\n",
            "  make/VBP\n",
            "  sen/NN\n",
            "  get/VB\n",
            "  big/JJ\n",
            "  cinemat/NNS\n",
            "  take/VBP\n",
            "  cinemat/NN\n",
            "  k/NN\n",
            "  whatev/VBP\n",
            "  feel/NN\n",
            "  like/IN\n",
            "  slow/JJ\n",
            "  motionthat/FW\n",
            "  friend/VBP\n",
            "  experi/JJ\n",
            "  watch/NN\n",
            "  jw/NN\n",
            "  style/NN\n",
            "  peopl/NN\n",
            "  applaud/NN\n",
            "  first/RB\n",
            "  movi/JJ\n",
            "  turn/NN\n",
            "  dement/JJ\n",
            "  caricatur/NN\n",
            "  fight/VBD\n",
            "  scene/NN\n",
            "  ridicul/NN\n",
            "  bad/JJ\n",
            "  action/NN\n",
            "  charact/NN\n",
            "  make/VBP\n",
            "  sen/JJ\n",
            "  world/NN\n",
            "  describ/NN\n",
            "  make/VBP\n",
            "  sen/VBN\n",
            "  even/RB\n",
            "  tongueincheek/JJ\n",
            "  refer/VBP\n",
            "  matrix/JJ\n",
            "  dont/NN\n",
            "  hit/VBD\n",
            "  know/JJ\n",
            "  enjoy/NN\n",
            "  film/NN\n",
            "  make/VBP\n",
            "  feel/NN\n",
            "  old/JJ\n",
            "  film/NN\n",
            "  three/CD\n",
            "  hour/NN\n",
            "  irrelevanceand/NN\n",
            "  among/IN\n",
            "  peopl/JJ\n",
            "  cant/JJ\n",
            "  understand/NN\n",
            "  rate/NN\n",
            "  film/NN\n",
            "  anyth/IN\n",
            "  bore/NN\n",
            "  watch/NN\n",
            "  end/NN\n",
            "  credit/NN\n",
            "  scene/NN\n",
            "  destroy/VBP\n",
            "  littl/NN\n",
            "  joy/NN\n",
            "  might/MD\n",
            "  kept/VB\n",
            "  watch/NN\n",
            "  thisbottom/NN\n",
            "  line/NN\n",
            "  mani/FW\n",
            "  talent/NN\n",
            "  peopl/NN\n",
            "  work/NN\n",
            "  film/NN\n",
            "  board/NN\n",
            "  imagin/NN\n",
            "  wor/VBP\n",
            "  outcom/RB\n",
            "  even/RB\n",
            "  funni/VBP\n",
            "  sad/JJ)\n",
            "Dependency Parsing Tree:\n",
            "('imagin', 'NN', 'O')\n",
            "('video', 'NN', 'O')\n",
            "('game', 'NN', 'O')\n",
            "('shoot', 'NN', 'O')\n",
            "('bad', 'JJ', 'O')\n",
            "('guy', 'NN', 'O')\n",
            "('hardwar', 'NN', 'O')\n",
            "('old', 'JJ', 'O')\n",
            "('everyth', 'NN', 'O')\n",
            "('kind', 'NN', 'O')\n",
            "('slow', 'JJ', 'O')\n",
            "('focu', 'NN', 'O')\n",
            "('oppon', 'NN', 'O')\n",
            "('set', 'VBN', 'O')\n",
            "('easi', 'JJ', 'O')\n",
            "('instal', 'JJ', 'O')\n",
            "('hack', 'NN', 'O')\n",
            "('give', 'VBP', 'O')\n",
            "('invinc', 'NN', 'O')\n",
            "('autoaim', 'NN', 'O')\n",
            "('come', 'VBP', 'O')\n",
            "('slowli', 'NN', 'O')\n",
            "('shout', 'NN', 'O')\n",
            "('open', 'JJ', 'O')\n",
            "('weapon', 'NN', 'O')\n",
            "('fire', 'NN', 'O')\n",
            "('three', 'CD', 'O')\n",
            "('four', 'CD', 'O')\n",
            "('bullet', 'NN', 'O')\n",
            "('run', 'VB', 'O')\n",
            "('shoot', 'JJ', 'O')\n",
            "('anyth', 'NN', 'O')\n",
            "('anyway', 'RB', 'O')\n",
            "('use', 'JJ', 'O')\n",
            "('sniper', 'JJ', 'O')\n",
            "('explo', 'NN', 'O')\n",
            "('trap', 'NN', 'O')\n",
            "('kind', 'NN', 'O')\n",
            "('cant', 'JJ', 'O')\n",
            "('even', 'RB', 'O')\n",
            "('hit', 'VBP', 'O')\n",
            "('theyr', 'JJ', 'O')\n",
            "('next', 'JJ', 'O')\n",
            "('wield', 'NN', 'O')\n",
            "('knife', 'NN', 'O')\n",
            "('tri', 'NN', 'O')\n",
            "('fight', 'NN', 'O')\n",
            "('decent', 'NN', 'O')\n",
            "('manner', 'NN', 'O')\n",
            "('yet', 'RB', 'O')\n",
            "('avatar', 'JJ', 'O')\n",
            "('move', 'NN', 'O')\n",
            "('like', 'IN', 'O')\n",
            "('year', 'NN', 'O')\n",
            "('old', 'JJ', 'O')\n",
            "('man', 'NN', 'O')\n",
            "('even', 'RB', 'O')\n",
            "('autoaim', 'VBZ', 'O')\n",
            "('still', 'RB', 'O')\n",
            "('suck', 'VBN', 'O')\n",
            "('somehow', 'RB', 'O')\n",
            "('get', 'VB', 'O')\n",
            "('mani', 'JJ', 'O')\n",
            "('separ', 'JJ', 'O')\n",
            "('level', 'NN', 'O')\n",
            "('noth', 'DT', 'O')\n",
            "('make', 'VBP', 'O')\n",
            "('sen', 'NN', 'O')\n",
            "('get', 'VB', 'O')\n",
            "('big', 'JJ', 'O')\n",
            "('cinemat', 'NNS', 'O')\n",
            "('take', 'VBP', 'O')\n",
            "('cinemat', 'NN', 'O')\n",
            "('k', 'NN', 'O')\n",
            "('whatev', 'VBP', 'O')\n",
            "('feel', 'NN', 'O')\n",
            "('like', 'IN', 'O')\n",
            "('slow', 'JJ', 'O')\n",
            "('motionthat', 'FW', 'O')\n",
            "('friend', 'VBP', 'O')\n",
            "('experi', 'JJ', 'O')\n",
            "('watch', 'NN', 'O')\n",
            "('jw', 'NN', 'O')\n",
            "('style', 'NN', 'O')\n",
            "('peopl', 'NN', 'O')\n",
            "('applaud', 'NN', 'O')\n",
            "('first', 'RB', 'O')\n",
            "('movi', 'JJ', 'O')\n",
            "('turn', 'NN', 'O')\n",
            "('dement', 'JJ', 'O')\n",
            "('caricatur', 'NN', 'O')\n",
            "('fight', 'VBD', 'O')\n",
            "('scene', 'NN', 'O')\n",
            "('ridicul', 'NN', 'O')\n",
            "('bad', 'JJ', 'O')\n",
            "('action', 'NN', 'O')\n",
            "('charact', 'NN', 'O')\n",
            "('make', 'VBP', 'O')\n",
            "('sen', 'JJ', 'O')\n",
            "('world', 'NN', 'O')\n",
            "('describ', 'NN', 'O')\n",
            "('make', 'VBP', 'O')\n",
            "('sen', 'VBN', 'O')\n",
            "('even', 'RB', 'O')\n",
            "('tongueincheek', 'JJ', 'O')\n",
            "('refer', 'VBP', 'O')\n",
            "('matrix', 'JJ', 'O')\n",
            "('dont', 'NN', 'O')\n",
            "('hit', 'VBD', 'O')\n",
            "('know', 'JJ', 'O')\n",
            "('enjoy', 'NN', 'O')\n",
            "('film', 'NN', 'O')\n",
            "('make', 'VBP', 'O')\n",
            "('feel', 'NN', 'O')\n",
            "('old', 'JJ', 'O')\n",
            "('film', 'NN', 'O')\n",
            "('three', 'CD', 'O')\n",
            "('hour', 'NN', 'O')\n",
            "('irrelevanceand', 'NN', 'O')\n",
            "('among', 'IN', 'O')\n",
            "('peopl', 'JJ', 'O')\n",
            "('cant', 'JJ', 'O')\n",
            "('understand', 'NN', 'O')\n",
            "('rate', 'NN', 'O')\n",
            "('film', 'NN', 'O')\n",
            "('anyth', 'IN', 'O')\n",
            "('bore', 'NN', 'O')\n",
            "('watch', 'NN', 'O')\n",
            "('end', 'NN', 'O')\n",
            "('credit', 'NN', 'O')\n",
            "('scene', 'NN', 'O')\n",
            "('destroy', 'VBP', 'O')\n",
            "('littl', 'NN', 'O')\n",
            "('joy', 'NN', 'O')\n",
            "('might', 'MD', 'O')\n",
            "('kept', 'VB', 'O')\n",
            "('watch', 'NN', 'O')\n",
            "('thisbottom', 'NN', 'O')\n",
            "('line', 'NN', 'O')\n",
            "('mani', 'FW', 'O')\n",
            "('talent', 'NN', 'O')\n",
            "('peopl', 'NN', 'O')\n",
            "('work', 'NN', 'O')\n",
            "('film', 'NN', 'O')\n",
            "('board', 'NN', 'O')\n",
            "('imagin', 'NN', 'O')\n",
            "('wor', 'VBP', 'O')\n",
            "('outcom', 'RB', 'O')\n",
            "('even', 'RB', 'O')\n",
            "('funni', 'VBP', 'O')\n",
            "('sad', 'JJ', 'O')\n",
            "\n",
            "Named Entity Recognition:\n",
            "Counter({('even', 'RB'): 4, ('make', 'VBP'): 4, ('film', 'NN'): 4, ('old', 'JJ'): 3, ('watch', 'NN'): 3, ('imagin', 'NN'): 2, ('bad', 'JJ'): 2, ('kind', 'NN'): 2, ('slow', 'JJ'): 2, ('three', 'CD'): 2, ('cant', 'JJ'): 2, ('like', 'IN'): 2, ('get', 'VB'): 2, ('feel', 'NN'): 2, ('peopl', 'NN'): 2, ('scene', 'NN'): 2, ('video', 'NN'): 1, ('game', 'NN'): 1, ('shoot', 'NN'): 1, ('guy', 'NN'): 1, ('hardwar', 'NN'): 1, ('everyth', 'NN'): 1, ('focu', 'NN'): 1, ('oppon', 'NN'): 1, ('set', 'VBN'): 1, ('easi', 'JJ'): 1, ('instal', 'JJ'): 1, ('hack', 'NN'): 1, ('give', 'VBP'): 1, ('invinc', 'NN'): 1, ('autoaim', 'NN'): 1, ('come', 'VBP'): 1, ('slowli', 'NN'): 1, ('shout', 'NN'): 1, ('open', 'JJ'): 1, ('weapon', 'NN'): 1, ('fire', 'NN'): 1, ('four', 'CD'): 1, ('bullet', 'NN'): 1, ('run', 'VB'): 1, ('shoot', 'JJ'): 1, ('anyth', 'NN'): 1, ('anyway', 'RB'): 1, ('use', 'JJ'): 1, ('sniper', 'JJ'): 1, ('explo', 'NN'): 1, ('trap', 'NN'): 1, ('hit', 'VBP'): 1, ('theyr', 'JJ'): 1, ('next', 'JJ'): 1, ('wield', 'NN'): 1, ('knife', 'NN'): 1, ('tri', 'NN'): 1, ('fight', 'NN'): 1, ('decent', 'NN'): 1, ('manner', 'NN'): 1, ('yet', 'RB'): 1, ('avatar', 'JJ'): 1, ('move', 'NN'): 1, ('year', 'NN'): 1, ('man', 'NN'): 1, ('autoaim', 'VBZ'): 1, ('still', 'RB'): 1, ('suck', 'VBN'): 1, ('somehow', 'RB'): 1, ('mani', 'JJ'): 1, ('separ', 'JJ'): 1, ('level', 'NN'): 1, ('noth', 'DT'): 1, ('sen', 'NN'): 1, ('big', 'JJ'): 1, ('cinemat', 'NNS'): 1, ('take', 'VBP'): 1, ('cinemat', 'NN'): 1, ('k', 'NN'): 1, ('whatev', 'VBP'): 1, ('motionthat', 'FW'): 1, ('friend', 'VBP'): 1, ('experi', 'JJ'): 1, ('jw', 'NN'): 1, ('style', 'NN'): 1, ('applaud', 'NN'): 1, ('first', 'RB'): 1, ('movi', 'JJ'): 1, ('turn', 'NN'): 1, ('dement', 'JJ'): 1, ('caricatur', 'NN'): 1, ('fight', 'VBD'): 1, ('ridicul', 'NN'): 1, ('action', 'NN'): 1, ('charact', 'NN'): 1, ('sen', 'JJ'): 1, ('world', 'NN'): 1, ('describ', 'NN'): 1, ('sen', 'VBN'): 1, ('tongueincheek', 'JJ'): 1, ('refer', 'VBP'): 1, ('matrix', 'JJ'): 1, ('dont', 'NN'): 1, ('hit', 'VBD'): 1, ('know', 'JJ'): 1, ('enjoy', 'NN'): 1, ('hour', 'NN'): 1, ('irrelevanceand', 'NN'): 1, ('among', 'IN'): 1, ('peopl', 'JJ'): 1, ('understand', 'NN'): 1, ('rate', 'NN'): 1, ('anyth', 'IN'): 1, ('bore', 'NN'): 1, ('end', 'NN'): 1, ('credit', 'NN'): 1, ('destroy', 'VBP'): 1, ('littl', 'NN'): 1, ('joy', 'NN'): 1, ('might', 'MD'): 1, ('kept', 'VB'): 1, ('thisbottom', 'NN'): 1, ('line', 'NN'): 1, ('mani', 'FW'): 1, ('talent', 'NN'): 1, ('work', 'NN'): 1, ('board', 'NN'): 1, ('wor', 'VBP'): 1, ('outcom', 'RB'): 1, ('funni', 'VBP'): 1, ('sad', 'JJ'): 1})\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('punkt')  # Add this line to download the 'punkt' resource\n",
        "nltk.download('words')\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "from nltk import pos_tag, ne_chunk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.tree import Tree\n",
        "from nltk.chunk import tree2conlltags\n",
        "from collections import Counter\n",
        "\n",
        "# ... (rest of your code remains the same)\n",
        "\n",
        "def pos_tagging(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    pos_counts = Counter(tag for word, tag in pos_tags)\n",
        "    return pos_tags, pos_counts\n",
        "\n",
        "def constituency_parsing(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence)\n",
        "        tagged = pos_tag(words)\n",
        "        parsing_tree = ne_chunk(tagged)\n",
        "        print(\"Constituency Parsing Tree:\")\n",
        "        print(parsing_tree)\n",
        "\n",
        "def dependency_parsing(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence)\n",
        "        tagged = pos_tag(words)\n",
        "        parsing_tree = ne_chunk(tagged)\n",
        "        conll_tags = tree2conlltags(parsing_tree)\n",
        "        print(\"Dependency Parsing Tree:\")\n",
        "        for tag in conll_tags:\n",
        "            print(tag)\n",
        "\n",
        "def named_entity_recognition(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    entities = []\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence)\n",
        "        tagged = pos_tag(words)\n",
        "        parsing_tree = ne_chunk(tagged, binary=True)\n",
        "        entities.extend([(word, entity) for word, entity, tag in tree2conlltags(parsing_tree) if entity != 'O'])\n",
        "    entity_counts = Counter(entities)\n",
        "    return entity_counts\n",
        "\n",
        "# Load cleaned reviews from the CSV file\n",
        "cleaned_reviews = []\n",
        "with open('imdb_reviews_cleaned.csv', 'r', encoding='utf-8') as csv_file:\n",
        "    reader = csv.reader(csv_file)\n",
        "    next(reader)  # Skip header\n",
        "    for row in reader:\n",
        "        cleaned_reviews.append(row[1])\n",
        "\n",
        "# Performing analyses on a sample review\n",
        "sample_review = cleaned_reviews[0]\n",
        "\n",
        "# (1) Parts of Speech (POS) Tagging\n",
        "pos_tags, pos_counts = pos_tagging(sample_review)\n",
        "print(\"\\nParts of Speech (POS) Tagging:\")\n",
        "print(pos_tags)\n",
        "print(\"POS Counts:\", pos_counts)\n",
        "\n",
        "# (2) Constituency Parsing and Dependency Parsing\n",
        "constituency_parsing(sample_review)\n",
        "dependency_parsing(sample_review)\n",
        "\n",
        "# (3) Named Entity Recognition\n",
        "entity_counts = named_entity_recognition(sample_review)\n",
        "print(\"\\nNamed Entity Recognition:\")\n",
        "print(entity_counts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWOtvT2rHNWy"
      },
      "source": [
        "**Write your explanations of the constituency parsing tree and dependency parsing tree here (Question 3-2):**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Constituency Parsing Tree:**\n",
        "Constituency parsing is a natural language parsing technique that divides a sentence into smaller\n",
        "constituents or phrases such as noun phrases (NP), verb phrases (VP), prepositional phrases (PP), and others.\n",
        "As a result, a hierarchical tree structure representing the syntactic structure of a sentence is produced.\n",
        "\n",
        "**Dependency Parsing Tree:**\n",
        "Dependency parsing is another approach for assessing a sentence's grammatical structure, but it concentrates on the relationships between words in the form of directed linkages (dependencies). Each word in the phrase is treated as a node, and the linkages between them indicate grammatical relationships.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FQzNClg3mqju"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ewQEVCaZmvvI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}