{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amulyabodempudi/amulya_INFO5731_Fall2023/blob/main/Bodempudi_Amulya_Exercise_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW5_oFVd9-pY"
      },
      "source": [
        "## The second In-class-exercise (09/13/2023, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kindly use the provided .ipynb document to write your code or respond to the questions. Avoid generating a new file.\n",
        "Execute all the cells before your final submission."
      ],
      "metadata": {
        "id": "mAzh1U0sE5I5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This in-class exercise is due tomorrow September 14, 2023 at 11:59 PM. No late submissions will be considered."
      ],
      "metadata": {
        "id": "PpgvZQdRE-HV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QBZI-je9-pZ"
      },
      "source": [
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWoKpYQT9-pa"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-LmNR3kw9-pa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "5d13f72e-03e4-4ba9-a024-3a5bd9e42853"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nPlease write you answer here:\\nQuestion:\\nExploring the Impact of Virtual Reality (VR) Therapy on Anxiety and Stress Reduction in Healthcare Settings\\n\\nData Collection:\\nTo answer these research questions, the following types of data need to be collected:\\nPatient Demographics: Collect data on patients' age, gender, medical history, and previous experiences with VR if any.\\nAnxiety Assessment: Use standardized anxiety assessment tools such as the State-Trait Anxiety Inventory (STAI) or the Hospital Anxiety and Depression Scale (HADS) to measure anxiety levels before and after VR therapy sessions.\\nVR Therapy Sessions: Record data on the type of VR scenarios used, session duration, and any adverse reactions or discomfort reported by patients during the sessions.\\nMedical Procedure Data: For patients undergoing medical procedures, collect data on the type of procedure, its duration, and any complications or difficulties experienced by healthcare providers during the procedure.\\nFollow-up Surveys: Administer follow-up surveys to patients after a designated period to assess the long-term impact of VR therapy on anxiety and stress levels.\\n\\nSample Size:\\nThe required sample size will depend on the statistical power desired and the effect size expected. To ensure meaningful results, aim for a sample size of at least 100 patients. A larger sample size may be necessary for subgroup analysis or to detect smaller effect sizes.\\n\\nData Collection Steps:\\n\\nEthical Approval: Obtain ethical approval from relevant institutional review boards or ethics committees.\\nRecruitment: Identify eligible patients in healthcare settings, such as hospitals or clinics, and obtain informed consent for participation in the study.\\nBaseline Data Collection: Collect baseline data on patient demographics and anxiety levels using standardized assessment tools.\\nVR Therapy Sessions: Administer VR therapy sessions according to a standardized protocol. Record session details and monitor patient reactions.\\nPost-Session Data: Collect post-session anxiety data immediately after the VR therapy sessions.\\nMedical Procedure Data: For patients undergoing medical procedures, collect data on the procedure and any relevant complications.\\nFollow-up Surveys: Administer follow-up surveys at predetermined intervals to assess the long-term impact of VR therapy.\\nData Storage: Ensure secure storage of collected data, adhering to data protection regulations.\\nData Analysis: Analyze the data using appropriate statistical methods, such as t-tests, ANOVA, regression analysis, or machine learning techniques, depending on the research questions and data types.\\nReporting: Summarize the findings in a research paper, including statistical results, conclusions, and recommendations.\\n\\nCollecting and analyzing this data will provide valuable insights into the effectiveness of VR therapy for anxiety and stress reduction in healthcare settings, potentially leading to innovative approaches for improving patient well-being.\\n\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "Question:\n",
        "Exploring the Impact of Virtual Reality (VR) Therapy on Anxiety and Stress Reduction in Healthcare Settings\n",
        "\n",
        "Data Collection:\n",
        "To answer these research questions, the following types of data need to be collected:\n",
        "Patient Demographics: Collect data on patients' age, gender, medical history, and previous experiences with VR if any.\n",
        "Anxiety Assessment: Use standardized anxiety assessment tools such as the State-Trait Anxiety Inventory (STAI) or the Hospital Anxiety and Depression Scale (HADS) to measure anxiety levels before and after VR therapy sessions.\n",
        "VR Therapy Sessions: Record data on the type of VR scenarios used, session duration, and any adverse reactions or discomfort reported by patients during the sessions.\n",
        "Medical Procedure Data: For patients undergoing medical procedures, collect data on the type of procedure, its duration, and any complications or difficulties experienced by healthcare providers during the procedure.\n",
        "Follow-up Surveys: Administer follow-up surveys to patients after a designated period to assess the long-term impact of VR therapy on anxiety and stress levels.\n",
        "\n",
        "Sample Size:\n",
        "The required sample size will depend on the statistical power desired and the effect size expected. To ensure meaningful results, aim for a sample size of at least 100 patients. A larger sample size may be necessary for subgroup analysis or to detect smaller effect sizes.\n",
        "\n",
        "Data Collection Steps:\n",
        "\n",
        "Ethical Approval: Obtain ethical approval from relevant institutional review boards or ethics committees.\n",
        "Recruitment: Identify eligible patients in healthcare settings, such as hospitals or clinics, and obtain informed consent for participation in the study.\n",
        "Baseline Data Collection: Collect baseline data on patient demographics and anxiety levels using standardized assessment tools.\n",
        "VR Therapy Sessions: Administer VR therapy sessions according to a standardized protocol. Record session details and monitor patient reactions.\n",
        "Post-Session Data: Collect post-session anxiety data immediately after the VR therapy sessions.\n",
        "Medical Procedure Data: For patients undergoing medical procedures, collect data on the procedure and any relevant complications.\n",
        "Follow-up Surveys: Administer follow-up surveys at predetermined intervals to assess the long-term impact of VR therapy.\n",
        "Data Storage: Ensure secure storage of collected data, adhering to data protection regulations.\n",
        "Data Analysis: Analyze the data using appropriate statistical methods, such as t-tests, ANOVA, regression analysis, or machine learning techniques, depending on the research questions and data types.\n",
        "Reporting: Summarize the findings in a research paper, including statistical results, conclusions, and recommendations.\n",
        "\n",
        "Collecting and analyzing this data will provide valuable insights into the effectiveness of VR therapy for anxiety and stress reduction in healthcare settings, potentially leading to innovative approaches for improving patient well-being.\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlxTLRNm9-pa"
      },
      "source": [
        "Question 2 (10 points): Write python code to collect 1000 data samples you discussed above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QpWOgjHi9-pa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f5f86dc-e8d0-4c7e-ddb9-f9cc9f7e566e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Patient_ID  Age  Gender Medical_History VR_Session_Type  Session_Duration  \\\n",
            "0           1   49   Other            None     Distraction                39   \n",
            "1           2   50    Male          Asthma     Therapeutic                24   \n",
            "2           3   55    Male          Asthma     Therapeutic                22   \n",
            "3           4   19  Female          Asthma     Therapeutic                28   \n",
            "4           5   32  Female    Hypertension     Distraction                49   \n",
            "\n",
            "   Pre_Session_Anxiety  Post_Session_Anxiety Procedure_Type  \\\n",
            "0                    0                     3           None   \n",
            "1                    7                     5           None   \n",
            "2                    6                     4           None   \n",
            "3                    9                     4           None   \n",
            "4                    5                     3           None   \n",
            "\n",
            "   Procedure_Duration Complications  \n",
            "0                 NaN          None  \n",
            "1                 NaN          None  \n",
            "2                 NaN          None  \n",
            "3                 NaN          None  \n",
            "4                 NaN          None  \n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "# Create an empty list to store data for each patient\n",
        "data_list = []\n",
        "\n",
        "# Simulate data collection for 1000 patients\n",
        "for patient_id in range(1, 1001):\n",
        "    # Simulate patient demographics\n",
        "    age = random.randint(18, 80)\n",
        "    gender = random.choice(['Male', 'Female', 'Other'])\n",
        "    medical_history = random.choice(['None', 'Hypertension', 'Diabetes', 'Asthma', 'Other'])\n",
        "\n",
        "    # Simulate VR session data\n",
        "    vr_session_type = random.choice(['Relaxation', 'Distraction', 'Therapeutic'])\n",
        "    session_duration = random.randint(10, 60)  # Minutes\n",
        "    pre_session_anxiety = random.randint(0, 10)\n",
        "    post_session_anxiety = random.randint(0, 10)\n",
        "\n",
        "    # Simulate medical procedure data for a subset of patients\n",
        "    if random.random() < 0.2:\n",
        "        procedure_type = random.choice(['Surgery', 'MRI', 'X-ray', 'Blood Test'])\n",
        "        procedure_duration = random.randint(20, 180)  # Minutes\n",
        "        complications = random.choice(['None', 'Minor', 'Major'])\n",
        "\n",
        "    else:\n",
        "        procedure_type = procedure_duration = complications = None\n",
        "\n",
        "    # Append data for the current patient as a dictionary to the list\n",
        "    data_list.append({\n",
        "        'Patient_ID': patient_id,\n",
        "        'Age': age,\n",
        "        'Gender': gender,\n",
        "        'Medical_History': medical_history,\n",
        "        'VR_Session_Type': vr_session_type,\n",
        "        'Session_Duration': session_duration,\n",
        "        'Pre_Session_Anxiety': pre_session_anxiety,\n",
        "        'Post_Session_Anxiety': post_session_anxiety,\n",
        "        'Procedure_Type': procedure_type,\n",
        "        'Procedure_Duration': procedure_duration,\n",
        "        'Complications': complications\n",
        "    })\n",
        "\n",
        "# Create a DataFrame by concatenating the list of patient data\n",
        "data = pd.DataFrame(data_list)\n",
        "\n",
        "# Save the collected data to a CSV file\n",
        "data.to_csv('research_data.csv', index=False)\n",
        "\n",
        "# Display the first few rows of the collected data\n",
        "print(data.head())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px6wgvog9-pa"
      },
      "source": [
        "Question 3 (10 points): Write python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"information retrieval\". The articles should be published in the last 10 years (2013-2023).\n",
        "\n",
        "The following information of the article needs to be collected:\n",
        "\n",
        "(1) Title\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "P5rjlclf9-pb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7921573d-5073-4b1e-86e1-8bf5bb7ab080"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Unable to retrieve page 59\n",
            "Article 1:\n",
            "Title: [PDF][PDF] Modern information retrieval: A brief overview\n",
            "Venue: A Singhal - IEEE Data Eng. Bull., 2001 - academia.edu\n",
            "Year: None\n",
            "Authors: None\n",
            "Abstract: None\n",
            "\n",
            "\n",
            "Article 2:\n",
            "Title: Information retrieval on the web\n",
            "Venue: M Kobayashi, K Takeda - ACM computing surveys (CSUR), 2000 - dl.acm.org\n",
            "Year: None\n",
            "Authors: None\n",
            "Abstract: None\n",
            "\n",
            "\n",
            "Article 3:\n",
            "Title: [BOOK][B] Modern information retrieval\n",
            "Venue: R Baeza-Yates, B Ribeiro-Neto - 1999 - people.ischool.berkeley.edu\n",
            "Year: None\n",
            "Authors: None\n",
            "Abstract: None\n",
            "\n",
            "\n",
            "Article 4:\n",
            "Title: [BOOK][B] Introduction to modern information retrieval\n",
            "Venue: GG Chowdhury - 2010 - books.google.com\n",
            "Year: None\n",
            "Authors: None\n",
            "Abstract: None\n",
            "\n",
            "\n",
            "Article 5:\n",
            "Title: [BOOK][B] Introduction to information retrieval\n",
            "Venue: H Schütze, CD Manning, P Raghavan - 2008 - cis.uni-muenchen.de\n",
            "Year: None\n",
            "Authors: None\n",
            "Abstract: None\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "\n",
        "# Function to scrape articles from Google Scholar\n",
        "def scrape_google_scholar(query, max_results=1000):\n",
        "    base_url = \"https://scholar.google.com/scholar\"\n",
        "    params = {\n",
        "        \"q\": query,\n",
        "        \"as_vis\": \"0\",  # Disable \"versions\" option\n",
        "        \"as_sdt\": \"0,5\",  # Sort by date (newest first)\n",
        "    }\n",
        "\n",
        "    articles = []\n",
        "    page = 0\n",
        "    while len(articles) < max_results:\n",
        "        params[\"start\"] = page * 10\n",
        "        response = requests.get(base_url, params=params)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "            results = soup.find_all(\"div\", class_=\"gs_ri\")\n",
        "\n",
        "            for result in results:\n",
        "                title = result.find(\"h3\").text\n",
        "                link = result.find(\"a\", href=True)[\"href\"]\n",
        "                venue = result.find(\"div\", class_=\"gs_a\").text\n",
        "                year = None\n",
        "                authors = None\n",
        "                abstract = None\n",
        "\n",
        "                # Extract year, authors, and abstract if available\n",
        "                extra_info = result.find(\"div\", class_=\"gs_rs\")\n",
        "                if extra_info:\n",
        "                    extra_info = extra_info.text.split(\" - \")\n",
        "                    if len(extra_info) >= 2:\n",
        "                        year = extra_info[0]\n",
        "                        authors = extra_info[1].split(\" - \")[0]\n",
        "                        if len(extra_info) > 2:\n",
        "                            abstract = \" - \".join(extra_info[2:])\n",
        "\n",
        "                articles.append({\n",
        "                    \"title\": title,\n",
        "                    \"venue\": venue,\n",
        "                    \"year\": year,\n",
        "                    \"authors\": authors,\n",
        "                    \"abstract\": abstract,\n",
        "                })\n",
        "\n",
        "            page += 1\n",
        "\n",
        "        else:\n",
        "            print(f\"Error: Unable to retrieve page {page + 1}\")\n",
        "            break\n",
        "\n",
        "        # To avoid overloading the server, add a delay between requests\n",
        "        time.sleep(1)\n",
        "\n",
        "    return articles[:max_results]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    keyword = \"information retrieval\"\n",
        "    max_articles = 1000\n",
        "    articles = scrape_google_scholar(keyword, max_articles)\n",
        "\n",
        "    # Print the first 5 articles as an example\n",
        "    for i, article in enumerate(articles[:5], start=1):\n",
        "        print(f\"Article {i}:\")\n",
        "        print(f\"Title: {article['title']}\")\n",
        "        print(f\"Venue: {article['venue']}\")\n",
        "        print(f\"Year: {article['year']}\")\n",
        "        print(f\"Authors: {article['authors']}\")\n",
        "        print(f\"Abstract: {article['abstract']}\")\n",
        "        print(\"\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do either of the question-4 tasks given below."
      ],
      "metadata": {
        "id": "yCQpbJnwTxAB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT3CNj_V9-pb"
      },
      "source": [
        "Question 4 (10 points): Write python code to collect 1000 posts from Twitter, or Facebook, or Instagram. You can either use hashtags, keywords, user_name, user_id, or other information to collect the data.\n",
        "\n",
        "The following information needs to be collected:\n",
        "\n",
        "(1) User_name\n",
        "\n",
        "(2) Posted time\n",
        "\n",
        "(3) Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FymVNKVi9-pb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "d999e5b9-adb3-4422-b921-0726a4e7f079"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "Forbidden",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mForbidden\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-cff25d7de10c>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_tweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeyword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"extended\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_tweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0muser_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscreen_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mposted_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreated_at\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0;31m# Reached end of current page, get the next page...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRawParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             model = ModelParser().parse(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tweepy/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpagination_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tweepy/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'payload_list'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpayload_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'payload_type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpayload_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpayload_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpayload_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpayload_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpayload_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tweepy/api.py\u001b[0m in \u001b[0;36msearch_tweets\u001b[0;34m(self, q, **kwargs)\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0m_Twitter\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mdocumentation\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mstandard\u001b[0m \u001b[0msearch\u001b[0m \u001b[0mAPI\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mdeveloper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtwitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcom\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0men\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtwitter\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0moverview\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \"\"\"\n\u001b[0;32m-> 1309\u001b[0;31m         return self.request(\n\u001b[0m\u001b[1;32m   1310\u001b[0m             'GET', 'search/tweets', endpoint_parameters=(\n\u001b[1;32m   1311\u001b[0m                 \u001b[0;34m'q'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'geocode'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lang'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'locale'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'result_type'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'count'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tweepy/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, endpoint, endpoint_parameters, params, headers, json_payload, parser, payload_list, payload_type, post_data, files, require_auth, return_cursors, upload_api, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mUnauthorized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m403\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mForbidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m404\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mNotFound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mForbidden\u001b[0m: 403 Forbidden\n453 - You currently have access to a subset of Twitter API v2 endpoints and limited v1.1 endpoints (e.g. media post, oauth) only. If you need access to this endpoint, you may need a different access level. You can learn more here: https://developer.twitter.com/en/portal/product"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "import tweepy\n",
        "\n",
        "# Set your Twitter API credentials\n",
        "consumer_key = \"ziUfa9kVnR26WXglyvjimtgri\"\n",
        "consumer_secret = \"xID6jqGP3QrLsyT7SAApgZznsluhSyOGkDLsREOdvhld29upps\"\n",
        "access_token = \"1621735299960250368-TRTprmTCIeE5yOWDktTnc0LSH0R3RT\"\n",
        "access_token_secret = \"VTu35BPYTsWCt6RO4QjW2v8izB8BaOLOFM7yW1Wu6a0up\"\n",
        "\n",
        "# Authenticate with Twitter API\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "\n",
        "# Create an API object\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "# Define the keyword you want to search for\n",
        "keyword = \"AP ELECTION\"\n",
        "max_tweets = 1000\n",
        "\n",
        "# Collect tweets\n",
        "tweets = []\n",
        "\n",
        "for tweet in tweepy.Cursor(api.search_tweets, q=keyword, lang=\"en\", tweet_mode=\"extended\").items(max_tweets):\n",
        "    user_name = tweet.user.screen_name\n",
        "    posted_time = tweet.created_at\n",
        "    text = tweet.full_text\n",
        "\n",
        "    tweets.append({\n",
        "        \"User_name\": user_name,\n",
        "        \"Posted_time\": posted_time,\n",
        "        \"Text\": text\n",
        "    })\n",
        "\n",
        "# Print the first 5 collected tweets as an example\n",
        "for i, tweet in enumerate(tweets[:5], start=1):\n",
        "    print(f\"Tweet {i}:\")\n",
        "    print(f\"User_name: {tweet['User_name']}\")\n",
        "    print(f\"Posted_time: {tweet['Posted_time']}\")\n",
        "    print(f\"Text: {tweet['Text']}\")\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4 (10 points):\n",
        "\n",
        "In this task, you are required to identify and utilize online tools for web scraping data from websites without the need for coding, with a specific focus on Parsehub. The objective is to gather data and save it in formats like CSV, Excel, or any other suitable file format.\n",
        "\n",
        "You have to mention an introduction to the tool which ever you prefer to use, steps to follow for web scrapping and the final output of the data collected.\n",
        "\n",
        "Upload a document (Word or PDF File) in the same repository and you can add the link in the ipynb file."
      ],
      "metadata": {
        "id": "wOeAr9TJTBgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the link to the document here\n",
        "https://github.com/amulyabodempudi/amulya_INFO5731_Fall2023/blob/main/Bodempudi_Amulya_exercise_2.pdf\n"
      ],
      "metadata": {
        "id": "N20TjXLmTG1u"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}